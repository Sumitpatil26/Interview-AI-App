# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M6tQyj6SFVJGxvbMTW8xk1N6PePeO8y8
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load training dataset
train_df = pd.read_csv('train.csv')

# Show the first few rows
train_df.head()

# Number of unique labels
unique_labels = train_df['Labels'].unique()
print(f"Number of unique labels: {len(unique_labels)}")
print("Unique labels:", unique_labels)

# Class distribution
label_counts = train_df['Labels'].value_counts()

# Bar plot
plt.figure(figsize=(12, 6))
sns.barplot(x=label_counts.index, y=label_counts.values)
plt.title("Class Distribution")
plt.ylabel("Count")
plt.xlabel("Labels")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import re
import nltk
import os
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Define the directory first
nltk_data_dir = os.path.join(os.getcwd(), "nltk_data")
os.makedirs(nltk_data_dir, exist_ok=True)

# Download required NLTK data
nltk.download("punkt", download_dir=nltk_data_dir)
nltk.download("stopwords", download_dir=nltk_data_dir)
nltk.download('punkt_tab', download_dir=nltk_data_dir)

# Set the path for nltk
nltk.data.path.append(nltk_data_dir)

stop_words = set(stopwords.words('english'))

def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    filtered_tokens = [word for word in tokens if word not in stop_words]
    return filtered_tokens

# Apply to the training data
train_df['clean_tokens'] = train_df['Interview Text'].apply(clean_text)

# Show sample
train_df[['Interview Text', 'clean_tokens']].head()

from collections import Counter
from wordcloud import WordCloud

def plot_wordcloud(label):
    all_words = train_df[train_df['Labels'] == label]['clean_tokens'].sum()
    word_freq = Counter(all_words)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Word Cloud for Class: {label}")
    plt.show()

# Example: Plot for first 3 classes
for label in unique_labels[:3]:
    plot_wordcloud(label)

import itertools

# Flatten all tokens
all_tokens = list(itertools.chain(*train_df['clean_tokens'].tolist()))
token_freq = Counter(all_tokens).most_common(20)

# Plot
words, freqs = zip(*token_freq)
plt.figure(figsize=(10,5))
sns.barplot(x=list(words), y=list(freqs))
plt.title("Top 20 Frequent Words in Training Data")
plt.xticks(rotation=45)
plt.ylabel("Frequency")
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt

# Rejoin tokens for TF-IDF input
train_df['clean_text'] = train_df['clean_tokens'].apply(lambda tokens: ' '.join(tokens))

# Load validation data
val_df = pd.read_csv('val.csv')
val_df['Interview Text'] = val_df['Interview Text'].astype(str)

# Preprocess validation set
val_df['clean_tokens'] = val_df['Interview Text'].apply(clean_text)
val_df['clean_text'] = val_df['clean_tokens'].apply(lambda tokens: ' '.join(tokens))

# Vectorize
tfidf = TfidfVectorizer(max_features=5000)
X_train = tfidf.fit_transform(train_df['clean_text'])
X_val = tfidf.transform(val_df['clean_text'])

y_train = train_df['Labels']
y_val = val_df['Labels']

# Train logistic regression
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_val)

# Evaluation
print("Logistic Regression Report:")
print(classification_report(y_val, y_pred_lr))
print("Accuracy:", accuracy_score(y_val, y_pred_lr))
print("F1 Score (weighted):", f1_score(y_val, y_pred_lr, average='weighted'))

# Confusion matrix
cm = confusion_matrix(y_val, y_pred_lr, labels=lr_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_model.classes_)
disp.plot(xticks_rotation=45)
plt.title("Logistic Regression Confusion Matrix")
plt.show()

import os
os.environ["WANDB_MODE"] = "disabled"
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
from sklearn.preprocessing import LabelEncoder
import torch
from datasets import Dataset

# Label encoding
label_encoder = LabelEncoder()
train_df['labels_enc'] = label_encoder.fit_transform(train_df['Labels'])
val_df['labels_enc'] = label_encoder.transform(val_df['Labels'])

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("sumit2603/bert-sports-interview-classifier")

def tokenize_function(example):
    return tokenizer(example["Interview Text"], truncation=True)

# Convert to HuggingFace Dataset
train_dataset = Dataset.from_pandas(train_df[['Interview Text', 'labels_enc']])
val_dataset = Dataset.from_pandas(val_df[['Interview Text', 'labels_enc']])

# Rename the 'labels_enc' column to 'labels'
train_dataset = train_dataset.rename_column('labels_enc', 'labels')
val_dataset = val_dataset.rename_column('labels_enc', 'labels')

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Model
num_labels = len(label_encoder.classes_)
model = BertForSequenceClassification.from_pretrained("sumit2603/bert-sports-interview-classifier", num_labels=num_labels)

# Training setup
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=lambda p: {
        'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1)),
        'f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='weighted')
    }
)

# Train BERT
trainer.train()

# Predictions
bert_preds = trainer.predict(val_dataset)
y_pred_bert = bert_preds.predictions.argmax(axis=-1)

print("BERT Classification Report:")
print(classification_report(val_df['labels_enc'], y_pred_bert, target_names=[str(cls) for cls in label_encoder.classes_]))

# Plot confusion matrix
cm = confusion_matrix(val_df['labels_enc'], y_pred_bert)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
disp.plot(xticks_rotation=45)
plt.title("BERT Confusion Matrix")
plt.show()

model.save_pretrained("./saved_bert_model")
tokenizer.save_pretrained("./saved_bert_model")

# Show top words contributing to each class
feature_names = tfidf.get_feature_names_out()
top_n = 10

for i, class_label in enumerate(lr_model.classes_):
    top_indices = lr_model.coef_[i].argsort()[-top_n:]
    print(f"Top features for class '{class_label}':")
    print([feature_names[j] for j in top_indices][::-1])
    print()

# Imports
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load GPT-2 model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Prompt template
def build_prompt(category, question):
    return f"Category: {category}\nQ: {question}\nA:"

# Text generation function
def generate_response(prompt, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_length,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).split("A:")[-1].strip()

# List of example interview prompts
examples = [
    ("post_game_reaction", "What are your thoughts on today‚Äôs performance?"),
    ("injury_update", "How is your recovery coming along?"),
    ("contract_talk", "Are you planning to re-sign with the team next season?"),
    ("team_dynamics", "How would you describe the current locker room atmosphere?"),
    ("playoff_expectations", "Do you feel prepared heading into the postseason?")
]

# Generate and display responses
for category, question in examples:
    prompt = build_prompt(category, question)
    response = generate_response(prompt)
    print(f"üóÇÔ∏è Interview Category: {category}")
    print(f"‚ùì Q: {question}")
    print(f"üí¨ A: {response}")
    print("-" * 70)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Reuse cleaned text
texts = train_df['clean_text']

# Step 1: TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=3000)
X = vectorizer.fit_transform(texts)

# Step 2: KMeans Clustering (you can tune n_clusters later)
n_clusters = 8
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
train_df['cluster'] = kmeans.fit_predict(X)

# Step 3: Reduce dimensions for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())

# Step 4: Plot clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=train_df['cluster'], palette='tab10')
plt.title("Interview Topic Clusters (PCA)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(title="Cluster")
plt.tight_layout()
plt.savefig("topic_clusters.png")
plt.show()

# Print cluster count
print(f"Total number of topic clusters: {n_clusters}")

import streamlit as st
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import numpy as np
import pickle

# Load tokenizer & model
@st.cache_resource
def load_classifier():
    tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
    model = BertForSequenceClassification.from_pretrained("./saved_bert_model")
    return tokenizer, model

tokenizer, model = load_classifier()

# Classification function
def classify_transcript(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    outputs = model(**inputs)
    pred_label = torch.argmax(outputs.logits, dim=1).item()
    return pred_label

from transformers import GPT2LMHeadModel, GPT2Tokenizer

@st.cache_resource
def load_gpt2():
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    return tokenizer, model

gpt2_tokenizer, gpt2_model = load_gpt2()

def generate_response(category, question):
    prompt = f"Category: {category}\nQ: {question}\nA:"
    inputs = gpt2_tokenizer(prompt, return_tensors="pt").to(gpt2_model.device)
    outputs = gpt2_model.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True).split("A:")[-1].strip()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE  # or use UMAP
import pandas as pd

# Use your original train_df['Interview Text'] and train_df['Label']
texts = train_df['Interview Text']
labels = train_df['Labels']

# TF-IDF
vectorizer = TfidfVectorizer(max_features=3000)
X = vectorizer.fit_transform(texts)

# t-SNE (or use UMAP if installed)
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X.toarray())

# Build DataFrame for plotting
embedding_df = pd.DataFrame({
    'x': X_tsne[:, 0],
    'y': X_tsne[:, 1],
    'label': labels,
    'sample_text': texts.str.slice(0, 100)  # Optional: shorten text for tooltip
})

# Save to CSV for Streamlit app
embedding_df.to_csv("embedding_data.csv", index=False)

import pandas as pd
import plotly.express as px

# Load pre-computed data (e.g., UMAP embeddings)
@st.cache_data
def load_embeddings():
    df = pd.read_csv("embedding_data.csv")  # should contain: x, y, label, sample_text
    return df

embeddings_df = load_embeddings()

def show_plot():
    fig = px.scatter(
        embeddings_df,
        x="x", y="y",
        color="label",
        hover_data=["sample_text"],
        title="Transcript Embedding Clusters"
    )
    st.plotly_chart(fig, use_container_width=True)

st.title("üèüÔ∏è Sports Interview AI Dashboard")

# Tabs or sections
section = st.sidebar.radio("Choose Feature", ["Transcript Classification", "Q&A Generator", "Visualization"])

if section == "Transcript Classification":
    st.header("üìå Classify Interview Transcript")
    text_input = st.text_area("Paste full transcript:")
    if st.button("Classify"):
        if text_input.strip():
            pred = classify_transcript(text_input)
            st.success(f"Predicted Category: {pred}")
        else:
            st.warning("Please enter transcript text.")

elif section == "Q&A Generator":
    st.header("üß† AI Interview Response Generator")
    category = st.selectbox("Select Category", ["post_game_reaction", "injury_update", "contract_talk", "team_dynamics", "playoff_expectations"])
    question = st.text_input("Enter your question:")
    if st.button("Generate Answer"):
        if question.strip():
            answer = generate_response(category, question)
            st.markdown("**üí¨ AI Response:**")
            st.write(answer)
        else:
            st.warning("Please enter a question.")

elif section == "Visualization":
    st.header("üìä Transcript Embedding Explorer")
    show_plot()

from huggingface_hub import login

# Log in with your Hugging Face token
login()

from transformers import BertForSequenceClassification, BertTokenizer
from huggingface_hub import HfApi

# Load from your saved folder
model = BertForSequenceClassification.from_pretrained("./saved_bert_model")
tokenizer = BertTokenizer.from_pretrained("./saved_bert_model")

# Replace with your model name
model_name = "bert-sports-interview-classifier"

# Push to Hugging Face Hub
model.push_to_hub(model_name)
tokenizer.push_to_hub(model_name)
